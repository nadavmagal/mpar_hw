{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Ex4_exercise_ver2_nadav.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xABx3A-soZMM"
      },
      "source": [
        "# **Mapping and Perception for an autonomous robot (0510-7951)**\n",
        "---\n",
        "* **Exercise 4-section 2: Detection, Segmentation and Multi Object Tracking**\n",
        "\n",
        "In this exercise we provide you with a baseline multi-object tracker on the [MOT16](https://motchallenge.net/data/MOT16/) dataset. Your task is to analize detection and tracking results based on different techniques from the lectures. \n",
        "We will use [MOT17Det Dataset](https://arxiv.org/pdf/1603.00831.pdf).See the [webpage](https://motchallenge.net/) for video sequences with ground truth annotation\n",
        "\n",
        "* **Goals**\n",
        "\n",
        "1. Experience with [**MOT challenge Dataset**](https://motchallenge.net/) and pytorch!\n",
        "\n",
        "2. Object Detection with [**Faster R-CNN** ]( https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch/)\n",
        "\n",
        "3. Instance segemenation based on [**Mask-RCNN**](https://arxiv.org/abs/1703.06870)\n",
        "\n",
        "4. Object Detection with YOLO5 [**YOLO5V5 in pytorch**]( https://pytorch.org/hub/ultralytics_yolov5/)\n",
        "\n",
        "5. Object Detection (Faster R-CNN) + Multiple Object(ID) Tracking with Simple Online and Realtime Tracking [**SORT**](https://arxiv.org/abs/1602.00763)\n",
        "\n",
        "6. Multi Target Tracking: Run given baseline of Multiple Object(ID) Tracking and compare to current state-of-the-art multi-object tracker [**Tracktor++**](https://arxiv.org/abs/1903.05625)\n",
        "\n",
        "\n",
        "* **Please copy to the report:**\n",
        "1. Outputs- Images, tables, scores,etc\n",
        "2. Code (\"TODO\" section)\n",
        "3. Performace, analysis and your explanations. \n",
        "4. Attach the completed notebook to the report package. \n",
        "* ## Setup\n",
        "\n",
        " Download and extract project data to your Google Drive\n",
        "\n",
        "1.   Install Google Drive on your desktop.\n",
        "2.   Save this notebook to your Google Drive by clicking `Save a copy in Drive` from the `File` menu.\n",
        "3.   Download attched **exercise.rar** file to your desktop and extract it into the `Colab Notebooks` folder in your Google Drive.\n",
        "4. Go to [**MOT 16 dataset**](https://motchallenge.net/data/MOT16/) ,at the bottom of the page go to download--> \"Get all data\"\n",
        "then Copy the MOT 16 dataset to \"\\exercise\\data\\MOT16\"\n",
        "5.   Wait until Google Drive finished the synchronisation. (This might take a while.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLi0z9lxVkkC"
      },
      "source": [
        "* **TODO**\n",
        "\n",
        "1. Student name: Nadav Magal\n",
        "\n",
        "2. ID: 303010326"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWELy8_1pDm9"
      },
      "source": [
        "#### Connect the notebook to your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1cVr12Cjpq2"
      },
      "source": [
        "# !pip freeze"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY4sb4bP7Wi8",
        "outputId": "4b4fdb8a-76f6-4b83-9ff5-7be2addb155e"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzPsBvk8jSl6"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JGDHuolLpWE"
      },
      "source": [
        "# terminal command\n",
        "# !wget -P /content/gdrive/MyDrive/MPAR/fourth_project/exercise/data https://motchallenge.net/data/MOT16.zip\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtKX5uTYOY2v"
      },
      "source": [
        "# !unzip /content/gdrive/MyDrive/MPAR/fourth_project/exercise/data/MOT16.zip -d /content/gdrive/MyDrive/MPAR/fourth_project/exercise/data/MOT16/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxKwhPl_Pa-y",
        "outputId": "e235fb5a-46a2-44a4-c4b4-f00d7799780b"
      },
      "source": [
        "!ls \"/content/gdrive/MyDrive/MPAR/fourth_project/exercise/data/MOT16/train\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MOT16-02  MOT16-04  MOT16-05  MOT16-09\tMOT16-10  MOT16-11  MOT16-13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CDNCYKuF0VD"
      },
      "source": [
        "root_dir = \"/content/gdrive/MyDrive/MPAR/fourth_project/exercise\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wme28GdFAFY"
      },
      "source": [
        "The `root_dir` path points to the directory and the content in your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5IArCFxEz-M",
        "outputId": "67a2acbc-c616-4405-a5e5-1778d5946c13"
      },
      "source": [
        "# !ls \"gdrive/My Drive/Colab Notebooks/perception\"\n",
        "# !ls \"gdrive/My Drive/Colab Notebooks/perception/src/tracker\"\n",
        "!ls -l \"/content/gdrive/MyDrive/MPAR/fourth_project/exercise\"\n",
        "!ls -l \"/content/gdrive/MyDrive/MPAR/fourth_project/exercise/src/tracker\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwx------ 3 root root 4096 Jun 27 15:51 data\n",
            "drwx------ 2 root root 4096 Jun 27 15:51 models\n",
            "drwx------ 2 root root 4096 Jun 27 15:51 output\n",
            "drwx------ 2 root root 4096 Jun 27 15:51 src\n",
            "total 35\n",
            "-rw------- 1 root root 10825 Jun 28 15:46 data_obj_detect.py\n",
            "-rw------- 1 root root  8516 Jun 28 15:39 data_track.py\n",
            "-rw------- 1 root root     0 Nov 20  2019 __init__.py\n",
            "-rw------- 1 root root   658 Nov 20  2019 object_detector.py\n",
            "drwx------ 2 root root  4096 Jun 27 15:51 __pycache__\n",
            "-rw------- 1 root root  1874 Nov 21  2019 tracker.py\n",
            "-rw------- 1 root root  8700 Jul  3 10:13 utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRMsynpFU6gh",
        "outputId": "11882220-08fb-4c53-d312-56a87b8aafb6"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.path.join(root_dir, 'src'))\n",
        "\n",
        "!pip install tqdm lap\n",
        "!pip install https://github.com/timmeinhardt/py-motmetrics/archive/fix_pandas_deprecating_warnings.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting lap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/64/d9fb6a75b15e783952b2fec6970f033462e67db32dc43dfbb404c14e91c2/lap-0.4.0.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: lap\n",
            "  Building wheel for lap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lap: filename=lap-0.4.0-cp37-cp37m-linux_x86_64.whl size=1590135 sha256=5a2c5bbd677b1e694e75c1eada6515def9dee57049d50c84d5c5bd4ce00e8fc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/3e/af/eddcd6ffaa27df8d0ddac573758f8953c4e57c64c4c8c8b7d0\n",
            "Successfully built lap\n",
            "Installing collected packages: lap\n",
            "Successfully installed lap-0.4.0\n",
            "Collecting https://github.com/timmeinhardt/py-motmetrics/archive/fix_pandas_deprecating_warnings.zip\n",
            "\u001b[?25l  Downloading https://github.com/timmeinhardt/py-motmetrics/archive/fix_pandas_deprecating_warnings.zip\n",
            "\u001b[K     \\ 225kB 800kB/s\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from motmetrics==1.1.3) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from motmetrics==1.1.3) (1.1.5)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from motmetrics==1.1.3) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.1->motmetrics==1.1.3) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.1->motmetrics==1.1.3) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.1->motmetrics==1.1.3) (1.15.0)\n",
            "Building wheels for collected packages: motmetrics\n",
            "  Building wheel for motmetrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for motmetrics: filename=motmetrics-1.1.3-cp37-none-any.whl size=134200 sha256=adbcf958d45a451b1c5d958cee8584cb84fe2c8fc712fbc561c377076e8d9c90\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s7x36tts/wheels/c9/e9/0a/79599df270874df0ab21a57a729e8a956aed3c744da3397efc\n",
            "Successfully built motmetrics\n",
            "Installing collected packages: motmetrics\n",
            "Successfully installed motmetrics-1.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGOohkAgo-hW",
        "outputId": "8356db35-f493-4404-824a-e1c91f89ef76"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tracker.data_track import MOT16Sequences\n",
        "from tracker.data_obj_detect import MOT16ObjDetect\n",
        "from tracker.object_detector import FRCNN_FPN ##FasterRCNN\n",
        "from tracker.tracker import Tracker\n",
        "from tracker.utils import (plot_sequence, evaluate_mot_accums, get_mot_accum,\n",
        "                           evaluate_obj_detect, obj_detect_transforms)\n",
        "\n",
        "import motmetrics as mm\n",
        "mm.lap.default_solver = 'lap'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1Qe2615tOQQ"
      },
      "source": [
        "# MOT16 dataset\n",
        "\n",
        "The MOT16 challenge provides 7 train and 7 test video sequences with multiple objects (pedestrians) per frame. It includes many challening scenarios with camera movement, high crowdedness and object occlusions. See the [webpage](https://motchallenge.net/data/MOT16/) for video sequences with ground truth annotation.\n",
        "\n",
        "The `MOT17Sequences` dataset class provides the possibilty to load single sequences, e.g., `seq_name = 'mot16_02'`, or the entire train/test set, e.g., `seq_name = 'mot16_train'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcbme23UQ5p2",
        "outputId": "19c78599-1197-4a44-83f4-04704b1c204b"
      },
      "source": [
        "# !ls \"gdrive/My Drive/Colab Notebooks/perception/data/MOT16/train\"\n",
        "# !ls \"gdrive/My Drive/Colab Notebooks/perception/data/MOT16/test\"\n",
        "!ls \"/content/gdrive/MyDrive/MPAR/fourth_project/exercise/data/MOT16/train\"\n",
        "!ls \"/content/gdrive/MyDrive/MPAR/fourth_project/exercise/data/MOT16/test\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MOT16-02  MOT16-04  MOT16-05  MOT16-09\tMOT16-10  MOT16-11  MOT16-13\n",
            "MOT16-01  MOT16-03  MOT16-06  MOT16-07\tMOT16-08  MOT16-12  MOT16-14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KEDZJVwQ6DX"
      },
      "source": [
        "In order to compare the tracking performance of different trackers without the effect of the object detector, the MOTChallenge provides a precomputed set of public object detections. Trackers are then evaluated on their capabilities to form tracks with the provided set. However, we want to allow you to improve on the object detections as well. Therefore, we participate in the MOT16 challenge with private detections.\n",
        "\n",
        "## Instance segmentations\n",
        "\n",
        "We provide the instance segmentations for the sequences `02`, `05`, `09` and `11`. These can be used for example to train a method which improves the bounding box position in occluded situations. See the original MOTS [webpage](https://www.vision.rwth-aachen.de/page/mots) for more info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rDoJXYBr0FL"
      },
      "source": [
        "from tracker.data_track import MOT16Sequences\n",
        "from tracker.data_obj_detect import MOT16ObjDetect\n",
        "from tracker.object_detector import FRCNN_FPN #FasterRCNN\n",
        "from tracker.tracker import Tracker"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAXZXvRHlPfP",
        "outputId": "2bd7f97f-06a7-407f-fe8f-1bc689b0a52f"
      },
      "source": [
        "seq_name = 'MOT16-05' #TODO: # selcect the Sequences number (#) which most close to the your final ID number\n",
        "data_dir = os.path.join(root_dir, 'data/MOT16')\n",
        "print(data_dir)\n",
        "sequences = MOT16Sequences(seq_name, data_dir, load_seg=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/MPAR/fourth_project/exercise/data/MOT16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy-XKWxEv4OM"
      },
      "source": [
        "1.1 TODO: Select an image for the sequences according to your final number in your ID X10 . copy results to the report\n",
        "\n",
        "1.2 TODO: Display the image + Bounding boxes(GT)\n",
        "\n",
        "1.3 TODO: Display the mask image of segmentation (GT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBRU3iE50SG4"
      },
      "source": [
        "seq = sequences[0]\n",
        "#1.1 TODO: Select an image for the sequences according to your final number in your ID X10 . copy results to the report\n",
        "frame = seq[60] # selcect image according to the final number in your ID X10\n",
        "img=frame['img']\n",
        "gt=frame['gt']\n",
        "# 1.2 TODO: Display the image + Bounding boxes(GT)\n",
        "# hint- gt_id, Bounding boxes in gt.items()\n",
        "img_np = img.mul(255).permute(1, 2, 0).byte().numpy()\n",
        "width, height, _ = img.shape\n",
        "\n",
        "dpi = 96\n",
        "fig, ax = plt.subplots(1, dpi=dpi)\n",
        "fig.set_size_inches(width / dpi, height / dpi)\n",
        "ax.set_axis_off()\n",
        "ax.imshow(img_np)\n",
        "\n",
        "for j, t in gt.items():\n",
        "  t_i = t\n",
        "  ax.add_patch(\n",
        "      plt.Rectangle(\n",
        "          (t_i[0], t_i[1]),\n",
        "          t_i[2] - t_i[0],\n",
        "          t_i[3] - t_i[1],\n",
        "          fill=False,\n",
        "          linewidth=1.0\n",
        "      ))\n",
        "\n",
        "  ax.annotate(j, (t_i[0] + (t_i[2] - t_i[0]) / 2.0, t_i[1] + (t_i[3] - t_i[1]) / 2.0),\n",
        "              weight='bold', fontsize=6, ha='center', va='center')\n",
        "\n",
        "plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# TODO# * (1.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecEDoiWf0xJ1"
      },
      "source": [
        "seg_img = frame['seg_img']\n",
        "# 1.3 TODO: Display the mask image of segmentation (GT)\n",
        "from skimage import io, color\n",
        "\n",
        "plt.figure()\n",
        "img_np = img.mul(255).permute(1, 2, 0).byte().numpy()\n",
        "io.imshow(color.label2rgb(seg_img, img_np, colors=[(255, 0, 0), (0, 0, 255), (0, 255, 0)], alpha=0.01, bg_label=0, bg_color=None))\n",
        "plt.show(block=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEO6RvP2uT9i"
      },
      "source": [
        "2.1 TODO: Implement instance segemenation based on maskRCNN (in PyTorch)\n",
        "\n",
        "2.2 Run and the same image (copy results to the report) and compare results to given instance segmenation GT \n",
        "\n",
        "Bonus- use IoU/Dice score metrics(+3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-qBep_C4pJc"
      },
      "source": [
        "import os\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "import random\n",
        "import PIL\n",
        "import torchvision\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "torch.set_grad_enabled(False)\n",
        "  \n",
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "plt.rcParams[\"axes.grid\"] = False\n",
        "\n",
        "#2.1 TODO: Implement instance segemenation based on maskRCNN (in PyTorch)\n",
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "model = model.eval().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUoqEhgtk4JX"
      },
      "source": [
        "coco_names = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA7OjLbbli72"
      },
      "source": [
        "COLORS = np.random.uniform(0, 255, size=(len(coco_names), 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozm7qRLekLb2"
      },
      "source": [
        "def get_outputs(image, model, threshold):\n",
        "    with torch.no_grad():\n",
        "        # forward pass of the image through the modle\n",
        "        outputs = model(image)\n",
        "    \n",
        "    # get all the scores\n",
        "    scores = list(outputs[0]['scores'].detach().cpu().numpy())\n",
        "    # index of those scores which are above a certain threshold\n",
        "    thresholded_preds_inidices = [scores.index(i) for i in scores if i > threshold]\n",
        "    thresholded_preds_count = len(thresholded_preds_inidices)\n",
        "    # get the masks\n",
        "    masks = (outputs[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
        "    # discard masks for objects which are below threshold\n",
        "    masks = masks[:thresholded_preds_count]\n",
        "    # get the bounding boxes, in (x1, y1), (x2, y2) format\n",
        "    boxes = [[(int(i[0]), int(i[1])), (int(i[2]), int(i[3]))]  for i in outputs[0]['boxes'].detach().cpu()]\n",
        "    # discard bounding boxes below threshold value\n",
        "    boxes = boxes[:thresholded_preds_count]\n",
        "    # get the classes labels\n",
        "    labels = [coco_names[i] for i in outputs[0]['labels']]\n",
        "    return masks, boxes, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ig1HGXrlON6"
      },
      "source": [
        "def draw_segmentation_map(image, masks, boxes, labels):\n",
        "    alpha = 1 \n",
        "    beta = 0.6 # transparency for the segmentation map\n",
        "    gamma = 0 # scalar added to each sum\n",
        "    for i in range(len(masks)):\n",
        "        red_map = np.zeros_like(masks[i]).astype(np.uint8)\n",
        "        green_map = np.zeros_like(masks[i]).astype(np.uint8)\n",
        "        blue_map = np.zeros_like(masks[i]).astype(np.uint8)\n",
        "        # apply a randon color mask to each object\n",
        "        color = COLORS[random.randrange(0, len(COLORS))]\n",
        "        red_map[masks[i] == 1], green_map[masks[i] == 1], blue_map[masks[i] == 1]  = color\n",
        "        # combine all the masks into a single image\n",
        "        segmentation_map = np.stack([red_map, green_map, blue_map], axis=2)\n",
        "        #convert the original PIL image into NumPy format\n",
        "        image = np.array(image)\n",
        "        # convert from RGN to OpenCV BGR format\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        # apply mask on the image\n",
        "        cv2.addWeighted(image, alpha, segmentation_map, beta, gamma, image)\n",
        "        # draw the bounding boxes around the objects\n",
        "        cv2.rectangle(image, boxes[i][0], boxes[i][1], color=color, \n",
        "                      thickness=2)\n",
        "        # put the label text above the objects\n",
        "        cv2.putText(image , labels[i], (boxes[i][0][0], boxes[i][0][1]-10), \n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, color, \n",
        "                    thickness=2, lineType=cv2.LINE_AA)\n",
        "    \n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ18-U2fEpJX"
      },
      "source": [
        "img=frame['img'];\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "#2.2 TODO: Run and the same image (copy results to the report) and compare results to given instance segmenation GT\n",
        "#hint use: torchvision.transforms.functional.to_tensor(pil_image_single).cuda()\n",
        "# https://debuggercafe.com/instance-segmentation-with-pytorch-and-mask-r-cnn/\n",
        "trans_to_pil = transforms.ToPILImage()\n",
        "image_pil = trans_to_pil(img)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "img = transform(image_pil)\n",
        "image = img.unsqueeze(0).cuda()\n",
        "\n",
        "masks, boxes, labels = get_outputs(image, model, 0.965)\n",
        "instance_seg_image = draw_segmentation_map(img_np, masks, boxes, labels)\n",
        "plt.figure()\n",
        "plt.imshow(instance_seg_image)\n",
        "plt.show(block=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cvaNe42gIll"
      },
      "source": [
        "# Object detector\n",
        "\n",
        "We provide you with an object detector pretrained on the MOT challenge training set. This detector can be used and improved to generate the framewise detections necessary for the subsequent tracking and data association step.\n",
        "\n",
        "The object detector is a [Faster R-CNN](https://arxiv.org/abs/1506.01497) with a Resnet50 feature extractor. We trained the native PyTorch implementation of Faster-RCNN. For more information check out the corresponding PyTorch [webpage](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).\n",
        "\n",
        "The pretrained Faster R-CNN ResNet-50 model that we are going to use expects the input image tensor to be in the form [n, c, h, w] and have a min size of 800px, where:\n",
        "\n",
        "n is the number of images\n",
        "c is the number of channels , for RGB images its 3\n",
        "h is the height of the image\n",
        "w is the width of the image\n",
        "The model will return\n",
        "\n",
        "Bounding boxes [x0, y0, x1, y1] all the predicted classes of shape (N,4) where N is the number of classes predicted by the model to be present in the image.\n",
        "Labels of all the predicted classes.\n",
        "Scores of each of the predicted label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzFdHZY_6DLf"
      },
      "source": [
        "# !ls \"gdrive/My Drive/Colab Notebooks/perception/models\"\n",
        "!ls \"/content/gdrive/MyDrive/MPAR/fourth_project/exercise/models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qubi7uE6EPd"
      },
      "source": [
        "\n",
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2g7oHASTa8v"
      },
      "source": [
        "obj_detect_model_file = os.path.join(root_dir, 'models/faster_rcnn_fpn.model')\n",
        "obj_detect_nms_thresh = 0.3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIXdzYMnMhiW"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# object detector\n",
        "obj_detect = FRCNN_FPN(num_classes=2, nms_thresh=obj_detect_nms_thresh)\n",
        "obj_detect_state_dict = torch.load(obj_detect_model_file,\n",
        "                                   map_location=lambda storage, loc: storage)\n",
        "obj_detect.load_state_dict(obj_detect_state_dict)\n",
        "obj_detect.eval()\n",
        "obj_detect.to(device)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwnMX8_GppU-"
      },
      "source": [
        "* **Faster R-CNN results**\n",
        "\n",
        "3.1 Run object detection (Faster R-CNN) on the same image and compare results to given detection GT \n",
        "\n",
        "Bonus: use IoU/Dice score metrics (+3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SiW4l69PKAQ"
      },
      "source": [
        "# TODO 3.1 Run object detection on the same image and compare results to given detection GT\n",
        "np_img_for_detection = img_np.copy()\n",
        "bboxes, scores = obj_detect.detect(image)\n",
        "print(bboxes)\n",
        "for cur_bbox in bboxes:\n",
        "  np_img_for_detection = cv2.rectangle(np_img_for_detection, (cur_bbox[0], cur_bbox[1]), (cur_bbox[2], cur_bbox[3]), (255,0,0), 2)\n",
        "plt.figure()\n",
        "plt.imshow(np_img_for_detection)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS3B32wE0o0q"
      },
      "source": [
        "In the next section, run the following evaluation of the object detection training set, you should obtain the following evaluation result:\n",
        "\n",
        "\n",
        "3.2 TODO: Copy your results to report\n",
        "\n",
        "AP:  Prec:  Rec:  TP:  FP: \n",
        "\n",
        "3.3 what can we learn from the performance?\n",
        "\n",
        "3.4 Describe pros & cons of the algorithm (at least one for each),based on attched the paper\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc6EXVCyBhtV"
      },
      "source": [
        "data_dir = os.path.join(root_dir, 'data/MOT16/train')\n",
        "dataset_test = MOT16ObjDetect(data_dir,\n",
        "                              obj_detect_transforms(train=False))\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "data_loader_test = DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=collate_fn)\n",
        "\n",
        "if False:\n",
        "  evaluate_obj_detect(obj_detect, data_loader_test)\n",
        "#3.2 TODO: Copy your results to report\n",
        "\n",
        "#AP: Prec: Rec: TP: FP:\n",
        "\n",
        "#3.3 what can we learn from the performance?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HP4JV8Ep6oP"
      },
      "source": [
        "#**YOLO5 Detector results**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSjLFyp3u0Y-"
      },
      "source": [
        "4.1 TODO: Implement object detection based on YOLOV5 in PyTorch (pretrained model). \n",
        "\n",
        "4.2 Run the algorithm on your image. \n",
        "\n",
        "what can we learn from the results? , Which detector architecture has better performance?\n",
        "\n",
        "4.3 Describe pros & cons of the algorithm (at least one for each). Use the attached paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9vh9ZcoX2kV"
      },
      "source": [
        "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTjbWLIEYO4B"
      },
      "source": [
        "import torch\n",
        "# 4.1 TODO: Implement object detection based on YOLOV5 in PyTorch (pretrained model).\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpIu8pOuef9n"
      },
      "source": [
        "# 4.2 Run the algorithm on your image.\n",
        "#TODO#\n",
        "results = model('/content/gdrive/MyDrive/MPAR/fourth_project/exercise/data/MOT16/train/MOT16-05/img1/000060.jpg')\n",
        "results.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4qAPscSH4f4"
      },
      "source": [
        "# Multi-object tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy-hAZjDH-dS"
      },
      "source": [
        "A. In this section you will run detection and MOT on \n",
        "your traning folder\n",
        "\n",
        "1. Object Detection with Faster R-CNN and save json file\n",
        "\n",
        "2. load the json file and run Multiple Object(ID) Tracking with Simple Online and Realtime Tracking(SORT) algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeOxN9ct_45_"
      },
      "source": [
        "from os.path import join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X571n_EfIouh"
      },
      "source": [
        "import sys\n",
        "MOT_PATH = '/content/gdrive/MyDrive/MPAR/fourth_project/exercise/data/MOT16'\n",
        "motdata = join(MOT_PATH,'train/MOT16-05/img1/')\n",
        "sys.path.append(motdata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEXxsDlLIo7A"
      },
      "source": [
        "\n",
        "import matplotlib.pylab as plt\n",
        "import cv2\n",
        "\n",
        "list_motdata = os.listdir(motdata)  \n",
        "list_motdata.sort()\n",
        "\n",
        "img_ex_path = motdata + list_motdata[0]\n",
        "img_ex_origin = cv2.imread(img_ex_path)\n",
        "img_ex = cv2.cvtColor(img_ex_origin, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(img_ex)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POdeYQyhJCL_"
      },
      "source": [
        "# Import required packages/modules first\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Sb6KHIhJKau"
      },
      "source": [
        "# Download the pretrained Faster R-CNN model from torchvision\n",
        "##TODO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, min_size=800)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za0Ay-p_JXxZ"
      },
      "source": [
        "# Define the class names given by PyTorch's official Docs\n",
        "\n",
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hac4E9RzJdgN"
      },
      "source": [
        "# Defining a function for get a prediction result from the model\n",
        "\n",
        "def get_prediction(img_path, threshold):\n",
        "  img = Image.open(img_path) # Load the image\n",
        "  transform = T.Compose([T.ToTensor()]) # Defing PyTorch Transform\n",
        "  img = transform(img) # Apply the transform to the image\n",
        "  pred = model([img]) # Pass the image to the model\n",
        "  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())] # Get the Prediction Score\n",
        "  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())] # Bounding boxes\n",
        "  pred_score = list(pred[0]['scores'].detach().numpy())\n",
        "  pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1] # Get list of index with score greater than threshold.\n",
        "  pred_boxes = pred_boxes[:pred_t+1]\n",
        "  pred_class = pred_class[:pred_t+1]\n",
        "  return pred_boxes, pred_class,pred_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmSX1yj6IUxH"
      },
      "source": [
        "Image is obtained from the image path\n",
        "The image is converted to image tensor using PyTorch’s Transforms\n",
        "The image is passed through the model to get the predictions\n",
        "Class, box coordinates are obtained, but only prediction score > threshold are chosen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y65nXB4iJkcL"
      },
      "source": [
        "# Defining a api function for object detection\n",
        "\n",
        "def object_detection_api(img_path, threshold=0.5, rect_th=3, text_size=1.5, text_th=3):\n",
        " \n",
        "    # TODO# = get_prediction(img_path, threshold) # Get predictions\n",
        "    img = cv2.imread(img_path)  # Read image with cv2\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "\n",
        "    boxes, pred_cls,pred_score = get_prediction(img_path, threshold)\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        cv2.rectangle(img, boxes[i][0], boxes[i][1], color=(0, 255, 0),\n",
        "                      thickness=rect_th)  # Draw Rectangle with the coordinates\n",
        "        cv2.putText(img, pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0, 255, 0),\n",
        "                    thickness=text_th)  # Write the prediction class\n",
        "    plt.figure(figsize=(15, 20))  # display the output image\n",
        "    plt.imshow(img)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show(block=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqswJqRiTSCp"
      },
      "source": [
        "prediction is obtained from get_prediction method\n",
        "for each prediction, bounding box is drawn and text is written\n",
        "with opencv\n",
        "the final image is displayed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Etg_3FzJzRv"
      },
      "source": [
        "# Example: After detection\n",
        "object_detection_api(img_ex_path,threshold=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg8whdpwJ3tT"
      },
      "source": [
        "The picture above is an example of applying Detection Network (in our case, Faster R-CNN).\n",
        "Since the purpose of dataset we are using is 'tracking', you can see that most of the detected classes are 'person'.\n",
        "We need a prediction result (bbs offset, class label, pred scores) for at least 100 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOnEhQjqJ4pF"
      },
      "source": [
        "# save json file \n",
        "json_detection_fp = r'/content/gdrive/MyDrive/MPAR/fourth_project/exercise/data/data.json'\n",
        "\n",
        "if False:\n",
        "  import glob\n",
        "  import json\n",
        "\n",
        "  json_dict = {}\n",
        "\n",
        "  path_to_data = join(MOT_PATH,'train/MOT16-05/img1/*jpg') # fill the correct path\n",
        "\n",
        "  for file_ in sorted(glob.glob(path_to_data)):\n",
        "    print('start processing ' + file_.split('/')[-1])\n",
        "    boxes, pred_cls, score = get_prediction(file_, threshold=0.8)\n",
        "    frames_list = []\n",
        "    for i in range(len(boxes)):\n",
        "      frames_list.append({'bbox' : [np.float64(boxes[i][0][0]), np.float64(boxes[i][0][1]), np.float64(boxes[i][1][0]) ,np.float64(boxes[i][1][1])],\n",
        "                          'labels' : COCO_INSTANCE_CATEGORY_NAMES.index(pred_cls[i]) ,\n",
        "                          'scores' : np.float64(score[i]) \n",
        "                          })\n",
        "    json_dict[file_.split('/')[-1]]  =  frames_list\n",
        "\n",
        "  with open(json_detection_fp, 'w') as outfile:\n",
        "      json.dump(json_dict, outfile,sort_keys=True)\n",
        "  print(json_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7r9qBmaKOSA"
      },
      "source": [
        "**Object ID Tracking with SORT**\n",
        "\n",
        "Simple Online and Realtime Tracking (SORT) algorithm for object ID tracking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnFssRpWAFZN"
      },
      "source": [
        "MOT_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cZa41YTKQSd"
      },
      "source": [
        "# Git clone: SORT Algorithm\n",
        "if False:\n",
        "  !cd \"{MOT_PATH}\";git clone https://github.com/abewley/sort.git\n",
        "    \n",
        "  sort = join(MOT_PATH,'sort/')\n",
        "  sys.path.append(sort)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9px1UoQRKTrs"
      },
      "source": [
        "# requirement for sort\n",
        "!cd \"{sort}\";pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RcHtiLwKZf6"
      },
      "source": [
        "# Optional: if error occurs, you might need to re-install scikit-image and imgaug\n",
        "if False:\n",
        "  !pip uninstall scikit-image\n",
        "  !pip uninstall imgaug\n",
        "  !pip install imgaug\n",
        "  !pip install -U scikit-image\n",
        "\n",
        "  import skimage\n",
        "  print(skimage.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icx8sqVDL3w4"
      },
      "source": [
        "print(json_detection_fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QsRmaydKdwA"
      },
      "source": [
        "# load json file\n",
        "import json\n",
        "import collections\n",
        "from pprint import pprint\n",
        "from sort import *\n",
        "\n",
        "jsonpath=json_detection_fp # load the saved json file\n",
        "with open(jsonpath) as data_file:    \n",
        "   data = json.load(data_file)\n",
        "odata = collections.OrderedDict(sorted(data.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a71xj0hAKlno"
      },
      "source": [
        "# Let's check out downloaded json file\n",
        "\n",
        "pprint(odata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEb9vwI3KuQ1"
      },
      "source": [
        "img_path = motdata    # img root path\n",
        "\n",
        "# Making new directory for saving results\n",
        "save_path = join(MOT_PATH,'save/')\n",
        "!mkdir \"{save_path}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbAQvGRtKxyG"
      },
      "source": [
        "mot_tracker = Sort()      # Tracker using SORT Algorithm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo9FrUaCK1dC"
      },
      "source": [
        "for key in odata.keys():   \n",
        "    arrlist = []\n",
        "    det_img = cv2.imread(os.path.join(img_path, key))\n",
        "    overlay = det_img.copy()\n",
        "    det_result = data[key] \n",
        "    \n",
        "    for info in det_result:\n",
        "        bbox = info['bbox']\n",
        "        labels = info['labels']\n",
        "        scores = info['scores']\n",
        "        templist = bbox+[scores]\n",
        "        \n",
        "        if labels == 1: # label 1 is a person in MS COCO Dataset\n",
        "            arrlist.append(templist)\n",
        "            \n",
        "    track_bbs_ids = mot_tracker.update(np.array(arrlist))\n",
        "    \n",
        "    mot_imgid = key.replace('.jpg','')\n",
        "    newname = save_path + mot_imgid + '_SORT.jpg'\n",
        "    print(mot_imgid)\n",
        "    \n",
        "    for j in range(track_bbs_ids.shape[0]):  \n",
        "        ele = track_bbs_ids[j, :]\n",
        "        x = int(ele[0])\n",
        "        y = int(ele[1])\n",
        "        x2 = int(ele[2])\n",
        "        y2 = int(ele[3])\n",
        "        track_label = str(int(ele[4])) \n",
        "        cv2.rectangle(det_img, (x, y), (x2, y2), (0, 255, 255), 4)\n",
        "        cv2.putText(det_img, '#'+track_label, (x+5, y-10), 0,0.6,(0,255,255),thickness=2)\n",
        "        \n",
        "    cv2.imwrite(newname,det_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqKo-jxMK9xo"
      },
      "source": [
        "It's all done!\n",
        "\n",
        "Finally, you can get a sequence of image with each Tracking ID for every detected person.\n",
        "\n",
        "Prepare 'MOT_SORT.gif' for demo experience (at least 100 frames)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGszq40utUfA"
      },
      "source": [
        "B. Baseline tracker and MOT analysis\n",
        "\n",
        "In this section we provide you with a simple baseline tracker which predicts object detections for each frame and generates tracks by assigning current detections to previous detections\n",
        "\n",
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQTAqnFptwul"
      },
      "source": [
        "seed = 12345\n",
        "seq_name = 'MOT16-05' # TODO fill the correct index\n",
        "data_dir = os.path.join(root_dir, 'data/MOT16')\n",
        "output_dir = os.path.join(root_dir, 'output')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5KgKxhmMm1r"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1eAxu_Op4Qn"
      },
      "source": [
        "use_dice = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV2c5yengHyC"
      },
      "source": [
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# dataset\n",
        "sequences = MOT16Sequences(seq_name, data_dir)\n",
        "\n",
        "# tracker\n",
        "class TrackerIoUAssignment(Tracker):\n",
        "\n",
        "    def data_association(self, boxes, scores):\n",
        "        if self.tracks:\n",
        "            track_ids = [t.id for t in self.tracks]\n",
        "            track_boxes = np.stack([t.box.numpy() for t in self.tracks], axis=0)\n",
        "            \n",
        "            distance = mm.distances.iou_matrix(track_boxes, boxes.numpy(), max_iou=0.5) \n",
        "            if use_dice:\n",
        "                distance = np.divide(2 * distance, distance + 1) # this is for dice\n",
        "            # print(distance)\n",
        "            # update existing tracks\n",
        "            remove_track_ids = []\n",
        "            for t, dist in zip(self.tracks, distance):\n",
        "                if np.isnan(dist).all():\n",
        "                    remove_track_ids.append(t.id)\n",
        "                else:\n",
        "                    match_id = np.nanargmin(dist)\n",
        "                    t.box = boxes[match_id]\n",
        "            self.tracks = [t for t in self.tracks\n",
        "                           if t.id not in remove_track_ids]\n",
        "\n",
        "            # add new tracks\n",
        "            new_boxes = []\n",
        "            new_scores = []\n",
        "            for i, dist in enumerate(np.transpose(distance)):\n",
        "                if np.isnan(dist).all():\n",
        "                    new_boxes.append(boxes[i])\n",
        "                    new_scores.append(scores[i])\n",
        "            self.add(new_boxes, new_scores)\n",
        "\n",
        "        else:\n",
        "            self.add(boxes, scores)\n",
        "        \n",
        "\n",
        "tracker = TrackerIoUAssignment(obj_detect)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvB1hRhWtwaE"
      },
      "source": [
        "## Run baseline tracker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG3QwqUXtVu2"
      },
      "source": [
        "time_total = 0\n",
        "mot_accums = []\n",
        "results_seq = {}\n",
        "for seq in sequences:\n",
        "    tracker.reset()\n",
        "    now = time.time()\n",
        "\n",
        "    print(f\"Tracking: {seq}\")\n",
        "\n",
        "    data_loader = DataLoader(seq, batch_size=1, shuffle=False)\n",
        "\n",
        "    for frame in tqdm(data_loader):\n",
        "        tracker.step(frame)\n",
        "    results = tracker.get_results()\n",
        "    results_seq[str(seq)] = results\n",
        "\n",
        "    if seq.no_gt:\n",
        "        print(f\"No GT evaluation data available.\")\n",
        "    else:\n",
        "        mot_accums.append(get_mot_accum(results, seq))\n",
        "\n",
        "    time_total += time.time() - now\n",
        "\n",
        "    print(f\"Tracks found: {len(results)}\")\n",
        "    print(f\"Runtime for {seq}: {time.time() - now:.1f} s.\")\n",
        "\n",
        "    if use_dice:\n",
        "        cur_output_dir = output_dir + '/dice/'\n",
        "    else:\n",
        "        cur_output_dir = output_dir\n",
        "\n",
        "    os.makedirs(cur_output_dir, exist_ok=True)\n",
        "    seq.write_results(results, os.path.join(cur_output_dir))\n",
        "    \n",
        "print(f\"Runtime for all sequences: {time_total:.1f} s.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvwfazTyvGuw"
      },
      "source": [
        "** Multi object tracker analysis **\n",
        "\n",
        "5.1  Go over the given baseline tracker, what is the main metric for the association bewteen the new detections and existing tracks?\n",
        "\n",
        "5.2   Open the saved txt file and explain the initial frames results according to [MOT challange paper](https://arxiv.org/pdf/1603.00831.pdf)  \n",
        "\n",
        "5.3 Analize the  tracking results based on  [**Evaluation Measures**](https://motchallenge.net/results/3D_MOT_2015/?chl=3&orderBy=IDF1&orderStyle=DESC&det=Public) metrics.  \n",
        "\n",
        "5.4  Change the given baseline tracker metric to Dice score, compare the tracking results to 5.3 .Which metric has better performance?\n",
        "\n",
        "5.5  Compare the tracking results (baseline tracker) to the given Tractor++ performance. Why do you think [**Tracktor++**](https://arxiv.org/abs/1903.05625) achieves better results? what are the main differences between the trackers (based on the paper)?\n",
        "\n",
        "5.6  Describe pros & cons of SORT and Tracktor++ trackers (at least pros & cons one for each). Use the attached papaer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm5oTrflegVn"
      },
      "source": [
        "evaluate_mot_accums(mot_accums,\n",
        "                     [str(s) for s in sequences if not s.no_gt],\n",
        "                     generate_overall=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR5oU3c-LeWg"
      },
      "source": [
        "The current state-of-the-art multi-object tracker [Tracktor++](https://arxiv.org/abs/1903.05625) achieves the following tracking results on the `MOT16-train` sequences:\n",
        "\n",
        "              IDF1   IDP   IDR  Rcll  Prcn  GT  MT  PT  ML  FP    FN IDs   FM  MOTA  MOTP\n",
        "    MOT16-02 45.8% 78.3% 32.4% 41.3% 99.8%  62   9  32  21  18 10909  59   68 40.9% 0.080\n",
        "    MOT16-04 71.1% 90.3% 58.6% 64.7% 99.8%  83  32  29  22  71 16785  22   29 64.5% 0.096\n",
        "    MOT16-05 64.0% 86.6% 50.7% 57.5% 98.1% 133  32  65  36  75  2942  37   59 55.8% 0.144\n",
        "    MOT16-09 54.6% 69.4% 45.0% 64.3% 99.1%  26  11  13   2  31  1903  22   31 63.3% 0.086\n",
        "    MOT16-10 64.3% 75.7% 55.9% 72.4% 98.0%  57  28  26   3 189  3543  71  125 70.4% 0.148\n",
        "    MOT16-11 63.3% 77.0% 53.7% 69.0% 98.9%  75  24  33  18  73  2924  26   26 68.0% 0.081\n",
        "    MOT16-13 73.6% 85.1% 64.8% 74.2% 97.6% 110  60  39  11 213  3000  62   90 71.9% 0.132\n",
        "    OVERALL  65.0% 84.0% 53.1% 62.6% 99.1% 546 196 237 113 670 42006 299  428 61.7% 0.106\n",
        "\n",
        "For your final submission you should focus on improving `MOTA`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7KPPxqG_5xt"
      },
      "source": [
        "## Visualize tracking results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq6tAiUBkBEk"
      },
      "source": [
        "plot_sequence(results_seq['MOT16-05'],\n",
        "              [s for s in sequences if str(s) == 'MOT16-05'][0],\n",
        "              first_n_frames=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STOcESYw1voD"
      },
      "source": [
        "# Notes \n",
        "\n",
        "*   Experiment and debug on a single train sequence. If something works on a single sequence evaluate all train sequences to check the generaliztion of your improvement.\n",
        "*   Remember to split the training set into multiple sets with different sequences if you train something and want to avoid overfitting.\n",
        "*   Sometimes the execution of a cell gets stuck. If this happends just abort the execution and restart the cell.\n",
        "*   If the notebook warns you that currently no GPU hardware acceleration is available, try again later and focus on some debugging or experiments than can be done only with the CPU.\n"
      ]
    }
  ]
}